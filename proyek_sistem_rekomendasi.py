# -*- coding: utf-8 -*-
"""Proyek Sistem Rekomendasi

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19HNGe7uJ3odwZGsFWcUJuUCPnmT9JzUC

# Import modul
Pada bagian ini kita akan mengimport semua modul yang akan digunakan dalam proyek kali ini
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.metrics.pairwise import cosine_similarity
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

"""Import API Kaggle untuk dapat mendowload dataset dari kaggle.com"""

from google.colab import files
files.upload()
! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json

"""# Data Loading
Pada bagian ini kita akan mengunduh dataset dari kaggle secara langsung, pada proyek kali ini kita menggunakan dataset tentang rekomendasi buku
"""

!kaggle datasets download CooperUnion/anime-recommendations-database
!unzip anime-recommendations-database.zip -d anime-recommendations-database

anime=pd.read_csv('anime-recommendations-database/anime.csv')
rating=pd.read_csv('anime-recommendations-database/rating.csv')

"""# EDA
Exploratory data analysis atau sering disingkat EDA merupakan proses investigasi awal pada data untuk menganalisis karakteristik, menemukan pola, anomali, dan memeriksa asumsi pada data.

## Anime

Memeriksa isi dari DataFrame buku
"""

anime.head()

"""Memeriksa jumlah dan tipe data setiap kolom"""

anime.info()

"""Memeriksa missing value"""

anime.isnull().sum()

"""Memeriksa data duplicated"""

anime.duplicated().sum()

"""Melihat Tipe anime terbanyak"""

per_tipe = anime['type'].value_counts().head(10)

# Plot grafik batang
plt.figure(figsize=(12,6))
per_tipe.plot(kind='bar', color='skyblue')
plt.xlabel("Tipe Anime")
plt.ylabel("Total Anime")
plt.title("Total anime per jenis")
plt.xticks(rotation=45)
plt.show()

"""Melihat anime dengan komunitas terbanyak"""

top_anime = anime[['name','members']].sort_values(by='members', ascending=False).head(10)
top_anime = top_anime[['name', 'members']]
print(top_anime.head())

plt.figure(figsize=(12,6))
plt.bar(top_anime['name'], top_anime['members'], color='skyblue')
plt.xlabel("Nama Anime")
plt.ylabel("Jumlah Anggota")
plt.title("Top 10 Anime berdasarkan Jumlah Anggota")
plt.xticks(rotation=45, ha='right')
plt.show()

"""## Rating

Memeriksa isi dari DataFrame 'rating'
"""

rating.head()

"""Memeriksa jumlah dan tipe data setiap kolom di 'buku'"""

rating.info()

"""Memeriksa deskripsi statistik"""

rating.describe(include='all')

"""Cek missing value"""

rating.isnull().sum()

"""cek data duplicated"""

rating.duplicated().sum()

"""Melihat jumlah user"""

user=rating['user_id'].nunique()
print(f'Jumlah user: {user}')

"""# Data Preparation
Pada Tahap ini kita akan mempersiapkan (preparation) dataset yang telah didefinikan pada tahap sebelumnya. tahap di mana kita melakukan proses transformasi pada data sehingga menjadi bentuk yang cocok untuk proses pemodelan. Ada beberapa tahapan yang umum dilakukan pada data preparation, antara lain, seleksi fitur, transformasi data, feature engineering, dan dimensionality reduction

## Content-Based Filtering

Membuat DataFrame baru dengan data dari anime, lalu lihat isinya
"""

df_fix= anime
df_fix.head()

"""Melihat tipe data"""

df_fix.info()

"""Mengatasi Missing Value"""

df_clean=df_fix.dropna()
df_clean.info()
data = df_clean

"""TF-IDF Vectorizer\
 Teknik tersebut juga akan digunakan pada sistem rekomendasi untuk menemukan representasi fitur penting dari setiap kategori penulis.
"""

tf = TfidfVectorizer()
tf.fit(data['genre'])
tf.get_feature_names_out()

"""Fit dan transformasi ke dalam bentuk matriks."""

tfidf_matrix = tf.fit_transform(data['genre'])

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

"""Mengubah vektor tf-idf dalam bentuk matriks menggunakan fungsi todense()"""

tfidf_matrix.todense()

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=data.name
).sample(47, axis=1).sample(10, axis=0)

"""## Collaborative Filtering

Membuat DataFrame baru dengan data dari rating, lalu lihat isinya
"""

df_fix2=rating
df_fix2.head()

"""Menghapus rating '-1'"""

df_fix2 = df_fix2[df_fix2['rating'] != -1]
df_fix2.head()

"""Encoding user_id"""

# Mengubah user_id menjadi list tanpa nilai yang sama
user_ids = df_fix2['user_id'].unique().tolist()
print('list userID: ', user_ids)

# Melakukan encoding user_id
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userID : ', user_to_user_encoded)

# Melakukan proses encoding angka ke ke user_id
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke userID: ', user_encoded_to_user)

"""Encoding anime_id"""

# Mengubah anime_id menjadi list tanpa nilai yang sama
anime_ids = df_fix2['anime_id'].unique().tolist()

# Melakukan proses encoding placeID
anime_to_anime_encoded = {x: i for i, x in enumerate(anime_ids)}

# Melakukan proses encoding angka ke placeID
anime_encoded_to_anime = {i: x for i, x in enumerate(anime_ids)}

"""Mapping user_id dan anime_id kedalam DataFrame"""

# Mapping user_id ke dataframe user
df_fix2['user'] = df_fix2['user_id'].map(user_to_user_encoded)

# Mapping anime_id ke dataframe anime
df_fix2['anime'] = df_fix2['anime_id'].map(anime_to_anime_encoded)

"""Cek beberapa hal dalam data seperti jumlah user, jumlah anime, dan mengubah nilai rating menjadi float"""

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)

# Mendapatkan jumlah anime
num_anime = len(anime_encoded_to_anime)
print(num_anime)

# Mengubah rating menjadi nilai float
df_fix2['rating'] = df_fix2['rating'].values.astype(np.float32)

# Nilai minimum rating
min_rating = min(df_fix2['rating'])

# Nilai maksimal rating
max_rating = max(df_fix2['rating'])

print('Jumlah user: {}, Jumlah anime: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_anime, min_rating, max_rating
))

"""Membagi train dan validation set"""

# Mengacak dataset
df_mod = df_fix2.sample(frac=1, random_state=42)
df_mod

"""Memetakan (mapping) data user dan anime menjadi satu value dan mengubah nilai rating menjadi dalam skala 0 sampai 1 agar mudah dalam melakukan proses training."""

# Membuat variabel x untuk mencocokkan data user dan anime menjadi satu value
x = df_mod[['user', 'anime']].values

# Membuat variabel y untuk membuat rating dari hasil
y = df_mod['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

"""Membagi train dan validation set dengan proporsi 80:20"""

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * df_mod.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""# Modeling

## Content-based Filtering

Membuat dataframe untuk melihat tf-idf matrix. Kolom diisi dengan genre anime dan baris diisi dengan nama anime

Cosine Similarity
"""

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

"""Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa nama anime"""

cosine_sim_df = pd.DataFrame(cosine_sim, index=data['name'], columns=data['name'])
print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix pada setiap anime
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""Sistem Rekomendasi"""

def anime_recommendations(nama_anime, similarity_data=cosine_sim_df,
                          items=data[['name','genre','type','rating']], k=10):
    """
    Rekomendasi Anime berdasarkan kemiripan dataframe

    Parameter:
    ---
    nama_anime : tipe data string (str)
                Nama Anime (index kemiripan dataframe)
    similarity_data : tipe data pd.DataFrame (object)
                      Kesamaan dataframe, simetrik, dengan anime sebagai
                      indeks dan kolom
    items : tipe data pd.DataFrame (object)
            Mengandung kedua nama dan fitur lainnya yang digunakan untuk mendefinisikan kemiripan
    k : tipe data integer (int)
        Banyaknya jumlah rekomendasi yang diberikan
    ---


    Pada index ini, kita mengambil k dengan nilai similarity terbesar
    pada index matrix yang diberikan (i).
    """


    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,nama_anime].to_numpy().argpartition(
        range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop nama_anime agar nama anime yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(nama_anime, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

"""Cek data anime yang akan kita gunakan"""

data[data.name.eq('Dotto Koni-chan')]

"""Menguji sistem rekomendasi"""

anime_recommendations('Dotto Koni-chan')

"""## Collaborative Filtering

Training
"""

class RecommenderNet(tf.keras.Model):
    def __init__(self, num_users, num_anime, embedding_size, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.num_users = num_users
        self.num_items = num_anime
        self.embedding_size = embedding_size

        self.user_embedding = layers.Embedding(
            input_dim=num_users,
            output_dim=embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.user_bias = layers.Embedding(num_users, 1)

        self.item_embedding = layers.Embedding(
            input_dim=num_anime,
            output_dim=embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.item_bias = layers.Embedding(num_anime, 1)

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        item_vector = self.item_embedding(inputs[:, 1])
        item_bias = self.item_bias(inputs[:, 1])

        dot_user_item = tf.reduce_sum(user_vector * item_vector, axis=1, keepdims=True)

        x = dot_user_item + user_bias + item_bias

        return tf.nn.sigmoid(x)

"""Inisiasi model"""

model = RecommenderNet(num_users, num_anime, 10)

"""Compile model"""

model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""Training model"""

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 1024,
    epochs = 10,
    validation_data = (x_val, y_val)
)

"""Visualisasi numerik"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""Rekomendasi Anime"""

anime_df = data
df = pd.read_csv('/content/anime-recommendations-database/rating.csv')

# Mengambil sample user
user_id = df.user_id.sample(1).iloc[0]
anime_visited_by_user = df[df.user_id == user_id]

# Operator bitwise (~), bisa diketahui di sini https://docs.python.org/3/reference/expressions.html
anime_not_visited = anime_df[~anime_df['anime_id'].isin(anime_visited_by_user.anime_id.values)]['anime_id']
anime_not_visited = list(
    set(anime_not_visited)
    .intersection(set(anime_to_anime_encoded.keys()))
)

anime_not_visited = [[anime_to_anime_encoded.get(x)] for x in anime_not_visited]
user_encoder = user_to_user_encoded.get(user_id)
user_anime_array = np.hstack(
    ([[user_encoder]] * len(anime_not_visited), anime_not_visited)
)

ratings = model.predict(user_anime_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_anime_ids = [
    anime_encoded_to_anime.get(anime_not_visited[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Anime with high ratings from user')
print('----' * 8)

top_anime_user = (
    anime_visited_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .anime_id.values
)

anime_df_rows = anime_df[anime_df['anime_id'].isin(top_anime_user)]
for row in anime_df_rows.itertuples():
    print(row.name, ':', row.genre)

print('----' * 8)
print('Top 10 anime recommendation')
print('----' * 8)

recommended_anime = anime_df[anime_df['anime_id'].isin(recommended_anime_ids)]
for row in recommended_anime.itertuples():
    print(row.name, ':', row.genre)

"""# Evaluasi

Matrix Mean Absolute Error (MAE)
"""

y_true = y_val
y_pred = model.predict(x_val)

mae = mean_absolute_error(y_true, y_pred)
print("MAE:", mae)

"""Matrix Root Mean Absolute Error"""

rmse = np.sqrt(mean_squared_error(y_true, y_pred))
print("RMSE:", rmse)